{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('F:/Data_Set/iris.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide dataset into dependent and independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.iloc[:,0:4].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "print(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset.iloc[:,-1].values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
      " 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
      " 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
      " 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
      " 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
      " 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
      " 'setosa' 'setosa' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print(y)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert categorical data into numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import OneHotEncoder \n",
    "#from sklearn.model_selection import Linearregression  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "from keras.optimizers import Adam\n",
    "import pydot \n",
    "import graphviz\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot  \n",
    "from keras.utils import plot_model \n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert categorical data into numerical form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y) \n",
    "y = pd.get_dummies(y).values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Dividing the dataset into traning set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create ANN Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(4,input_shape=(4,),activation='relu')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(3,activation='softmax')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Compile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'Adam',loss='categorical_crossentropy',metrics = ['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finel ephoc or traning stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 1.1191 - accuracy: 0.3667\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 0s 217us/step - loss: 1.0436 - accuracy: 0.3667\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 0s 217us/step - loss: 0.9857 - accuracy: 0.3750\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 0s 200us/step - loss: 0.9369 - accuracy: 0.3917\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 0s 225us/step - loss: 0.9015 - accuracy: 0.5250\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 0s 258us/step - loss: 0.8765 - accuracy: 0.6500\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 0s 283us/step - loss: 0.8585 - accuracy: 0.6583\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 0s 242us/step - loss: 0.8474 - accuracy: 0.6667\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 0s 233us/step - loss: 0.8354 - accuracy: 0.6833\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 0s 233us/step - loss: 0.8248 - accuracy: 0.6833\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 0s 275us/step - loss: 0.8152 - accuracy: 0.6917\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 0s 217us/step - loss: 0.8054 - accuracy: 0.6917\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 0s 258us/step - loss: 0.7961 - accuracy: 0.6917\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 0s 225us/step - loss: 0.7859 - accuracy: 0.6917\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.6905 - accuracy: 0.80 - 0s 267us/step - loss: 0.7759 - accuracy: 0.6917\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 0s 250us/step - loss: 0.7662 - accuracy: 0.6917\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 0s 275us/step - loss: 0.7562 - accuracy: 0.6917\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 0s 250us/step - loss: 0.7461 - accuracy: 0.6917\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 0s 208us/step - loss: 0.7353 - accuracy: 0.6917\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 0s 258us/step - loss: 0.7254 - accuracy: 0.6917\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 0s 250us/step - loss: 0.7146 - accuracy: 0.6917\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 0s 250us/step - loss: 0.7051 - accuracy: 0.6917\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 0s 233us/step - loss: 0.6940 - accuracy: 0.6917\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 0s 250us/step - loss: 0.6847 - accuracy: 0.6917\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 0s 333us/step - loss: 0.6740 - accuracy: 0.6917\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.6636 - accuracy: 0.6917\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 0s 92us/step - loss: 0.6540 - accuracy: 0.6917\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.6448 - accuracy: 0.6917\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.6351 - accuracy: 0.6917\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.6261 - accuracy: 0.6917\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.6171 - accuracy: 0.6917\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 0s 270us/step - loss: 0.6088 - accuracy: 0.6917\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 0s 250us/step - loss: 0.6001 - accuracy: 0.6917\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 0s 217us/step - loss: 0.5921 - accuracy: 0.6917\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 0s 233us/step - loss: 0.5843 - accuracy: 0.6917\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 0s 250us/step - loss: 0.5768 - accuracy: 0.6917\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4858 - accuracy: 0.80 - 0s 300us/step - loss: 0.5698 - accuracy: 0.6917\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 0s 233us/step - loss: 0.5627 - accuracy: 0.6917\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 0s 225us/step - loss: 0.5570 - accuracy: 0.6917\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 0s 342us/step - loss: 0.5502 - accuracy: 0.6917\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 0s 258us/step - loss: 0.5434 - accuracy: 0.6917\n",
      "Epoch 42/100\n",
      "120/120 [==============================] - 0s 275us/step - loss: 0.5368 - accuracy: 0.6917\n",
      "Epoch 43/100\n",
      "120/120 [==============================] - 0s 283us/step - loss: 0.5272 - accuracy: 0.6917\n",
      "Epoch 44/100\n",
      "120/120 [==============================] - 0s 242us/step - loss: 0.5153 - accuracy: 0.6917\n",
      "Epoch 45/100\n",
      "120/120 [==============================] - 0s 233us/step - loss: 0.5071 - accuracy: 0.6917\n",
      "Epoch 46/100\n",
      "120/120 [==============================] - 0s 242us/step - loss: 0.4976 - accuracy: 0.6917\n",
      "Epoch 47/100\n",
      "120/120 [==============================] - 0s 267us/step - loss: 0.4897 - accuracy: 0.6917\n",
      "Epoch 48/100\n",
      "120/120 [==============================] - 0s 217us/step - loss: 0.4812 - accuracy: 0.6917\n",
      "Epoch 49/100\n",
      "120/120 [==============================] - 0s 200us/step - loss: 0.4736 - accuracy: 0.6917\n",
      "Epoch 50/100\n",
      "120/120 [==============================] - 0s 208us/step - loss: 0.4659 - accuracy: 0.6917\n",
      "Epoch 51/100\n",
      "120/120 [==============================] - 0s 242us/step - loss: 0.4594 - accuracy: 0.6917\n",
      "Epoch 52/100\n",
      "120/120 [==============================] - 0s 250us/step - loss: 0.4522 - accuracy: 0.6917\n",
      "Epoch 53/100\n",
      "120/120 [==============================] - 0s 250us/step - loss: 0.4477 - accuracy: 0.7000\n",
      "Epoch 54/100\n",
      "120/120 [==============================] - 0s 233us/step - loss: 0.4389 - accuracy: 0.7000\n",
      "Epoch 55/100\n",
      "120/120 [==============================] - 0s 233us/step - loss: 0.4339 - accuracy: 0.7083\n",
      "Epoch 56/100\n",
      "120/120 [==============================] - 0s 250us/step - loss: 0.4271 - accuracy: 0.7333\n",
      "Epoch 57/100\n",
      "120/120 [==============================] - 0s 275us/step - loss: 0.4212 - accuracy: 0.7500\n",
      "Epoch 58/100\n",
      "120/120 [==============================] - 0s 258us/step - loss: 0.4162 - accuracy: 0.7500\n",
      "Epoch 59/100\n",
      "120/120 [==============================] - 0s 250us/step - loss: 0.4125 - accuracy: 0.7917\n",
      "Epoch 60/100\n",
      "120/120 [==============================] - 0s 200us/step - loss: 0.4051 - accuracy: 0.8167\n",
      "Epoch 61/100\n",
      "120/120 [==============================] - 0s 210us/step - loss: 0.4010 - accuracy: 0.8083\n",
      "Epoch 62/100\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.3948 - accuracy: 0.8167\n",
      "Epoch 63/100\n",
      "120/120 [==============================] - 0s 200us/step - loss: 0.3915 - accuracy: 0.8583\n",
      "Epoch 64/100\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.3858 - accuracy: 0.8750\n",
      "Epoch 65/100\n",
      "120/120 [==============================] - 0s 208us/step - loss: 0.3815 - accuracy: 0.8750\n",
      "Epoch 66/100\n",
      "120/120 [==============================] - 0s 208us/step - loss: 0.3777 - accuracy: 0.8833\n",
      "Epoch 67/100\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.3723 - accuracy: 0.8833\n",
      "Epoch 68/100\n",
      "120/120 [==============================] - 0s 217us/step - loss: 0.3679 - accuracy: 0.8750\n",
      "Epoch 69/100\n",
      "120/120 [==============================] - 0s 242us/step - loss: 0.3643 - accuracy: 0.8833\n",
      "Epoch 70/100\n",
      "120/120 [==============================] - 0s 225us/step - loss: 0.3601 - accuracy: 0.8917\n",
      "Epoch 71/100\n",
      "120/120 [==============================] - 0s 250us/step - loss: 0.3560 - accuracy: 0.9083\n",
      "Epoch 72/100\n",
      "120/120 [==============================] - 0s 242us/step - loss: 0.3519 - accuracy: 0.9167\n",
      "Epoch 73/100\n",
      "120/120 [==============================] - 0s 217us/step - loss: 0.3487 - accuracy: 0.9167\n",
      "Epoch 74/100\n",
      "120/120 [==============================] - 0s 225us/step - loss: 0.3444 - accuracy: 0.9167\n",
      "Epoch 75/100\n",
      "120/120 [==============================] - 0s 225us/step - loss: 0.3423 - accuracy: 0.9167\n",
      "Epoch 76/100\n",
      "120/120 [==============================] - 0s 225us/step - loss: 0.3372 - accuracy: 0.9167\n",
      "Epoch 77/100\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.3348 - accuracy: 0.9333\n",
      "Epoch 78/100\n",
      "120/120 [==============================] - 0s 225us/step - loss: 0.3306 - accuracy: 0.9250\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 183us/step - loss: 0.3275 - accuracy: 0.9333\n",
      "Epoch 80/100\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.3241 - accuracy: 0.9333\n",
      "Epoch 81/100\n",
      "120/120 [==============================] - 0s 217us/step - loss: 0.3212 - accuracy: 0.9417\n",
      "Epoch 82/100\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.3192 - accuracy: 0.9250\n",
      "Epoch 83/100\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.3144 - accuracy: 0.9417\n",
      "Epoch 84/100\n",
      "120/120 [==============================] - 0s 217us/step - loss: 0.3114 - accuracy: 0.9417\n",
      "Epoch 85/100\n",
      "120/120 [==============================] - 0s 225us/step - loss: 0.3094 - accuracy: 0.9417\n",
      "Epoch 86/100\n",
      "120/120 [==============================] - 0s 250us/step - loss: 0.3059 - accuracy: 0.9417\n",
      "Epoch 87/100\n",
      "120/120 [==============================] - 0s 317us/step - loss: 0.3045 - accuracy: 0.9417\n",
      "Epoch 88/100\n",
      "120/120 [==============================] - 0s 225us/step - loss: 0.3011 - accuracy: 0.9417\n",
      "Epoch 89/100\n",
      "120/120 [==============================] - 0s 208us/step - loss: 0.2984 - accuracy: 0.9417\n",
      "Epoch 90/100\n",
      "120/120 [==============================] - 0s 208us/step - loss: 0.2950 - accuracy: 0.9417\n",
      "Epoch 91/100\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.2926 - accuracy: 0.9417\n",
      "Epoch 92/100\n",
      "120/120 [==============================] - 0s 233us/step - loss: 0.2897 - accuracy: 0.9417\n",
      "Epoch 93/100\n",
      "120/120 [==============================] - 0s 208us/step - loss: 0.2875 - accuracy: 0.9417\n",
      "Epoch 94/100\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.2844 - accuracy: 0.9417\n",
      "Epoch 95/100\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.2825 - accuracy: 0.9417\n",
      "Epoch 96/100\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.2809 - accuracy: 0.9417\n",
      "Epoch 97/100\n",
      "120/120 [==============================] - 0s 200us/step - loss: 0.2780 - accuracy: 0.9500\n",
      "Epoch 98/100\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.2771 - accuracy: 0.9417\n",
      "Epoch 99/100\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.2724 - accuracy: 0.9500\n",
      "Epoch 100/100\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.2725 - accuracy: 0.9500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1da47058470>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,batch_size=10,nb_epoch=100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_test_class = np.argmax(y_test,axis=1)\n",
    "y_pred_class = np.argmax(y_pred,axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "[[11  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0  6]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test_class,y_pred_class)) \n",
    "print(confusion_matrix(y_test_class,y_pred_class)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(12+5+8)/30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(11+13+6)/30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
