{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import io "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('F:/Data_Set/cancer_datasets.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign = 357\n",
      "Malignanat = 212\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAASRklEQVR4nO3df7BfdX3n8efLBIWptMDm6sYkNq6brkVbg14pW2e3FLsr0u2CDjphpjV1mYmdwR3tdDqF7qxau8zqFsuobZkJ5adjVUa0pA51i1TqOlbwwsYQQMasUonJwlWRH1LZSfreP77nfvxy803yBXK+30vu8zFz5nvO53zO+b4vE+7rfj7nfM83VYUkSQDPmXYBkqSlw1CQJDWGgiSpMRQkSY2hIElqVk67gGdi1apVtX79+mmXIUnPKrfffvt3q2pm1L5ndSisX7+eubm5aZchSc8qSf7hYPucPpIkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1z+pPNEtHs2+/7+emXYKWoBe/+85ez9/bSCHJsUluS/K1JHcl+YOu/eok30qyvVs2du1J8uEku5LsSPKqvmqTJI3W50jhCeCMqnosyTHAl5L8dbfvd6vqU4v6vwHY0C2/AFzWvUqSJqS3kUINPNZtHtMth/pC6LOBa7vjvgKckGR1X/VJkg7U64XmJCuSbAceBG6qqlu7XRd3U0SXJnle17YGuH/o8N1d2+Jzbkkyl2Rufn6+z/IladnpNRSqan9VbQTWAqcmeQVwEfAy4DXAScDvdd0z6hQjzrm1qmaranZmZuTjwCVJT9NEbkmtqh8AtwBnVtXeboroCeAq4NSu225g3dBha4E9k6hPkjTQ591HM0lO6NaPA34F+PrCdYIkAc4BdnaHbAPe2t2FdBrwcFXt7as+SdKB+rz7aDVwTZIVDMLnuqr6bJK/TTLDYLpoO/BbXf8bgbOAXcDjwNt6rE2SNEJvoVBVO4BTRrSfcZD+BVzQVz2SpMPzMReSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJTW+hkOTYJLcl+VqSu5L8Qdf+kiS3JvlGkk8meW7X/rxue1e3f31ftUmSRutzpPAEcEZVvRLYCJyZ5DTgA8ClVbUBeAg4v+t/PvBQVf1L4NKunyRpgnoLhRp4rNs8plsKOAP4VNd+DXBOt352t023/3VJ0ld9kqQD9XpNIcmKJNuBB4GbgP8D/KCq9nVddgNruvU1wP0A3f6HgX824pxbkswlmZufn++zfEladnoNharaX1UbgbXAqcDPjurWvY4aFdQBDVVbq2q2qmZnZmaOXLGSpMncfVRVPwBuAU4DTkiystu1FtjTre8G1gF0+38K+P4k6pMkDfR599FMkhO69eOAXwHuAb4AnNt12wzc0K1v67bp9v9tVR0wUpAk9Wfl4bs8bauBa5KsYBA+11XVZ5PcDXwiyX8D/jdwRdf/CuCjSXYxGCFs6rE2SdIIvYVCVe0AThnR/k0G1xcWt/8IeHNf9UiSDs9PNEuSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1vYVCknVJvpDkniR3JXln1/7eJN9Jsr1bzho65qIku5Lcm+T1fdUmSRptZY/n3gf8TlXdkeR44PYkN3X7Lq2qS4Y7JzkZ2AS8HHgR8PkkP1NV+3usUZI0pLeRQlXtrao7uvVHgXuANYc45GzgE1X1RFV9C9gFnNpXfZKkA03kmkKS9cApwK1d0zuS7EhyZZITu7Y1wP1Dh+1mRIgk2ZJkLsnc/Px8j1VL0vLTeygkeT5wPfCuqnoEuAx4KbAR2At8cKHriMPrgIaqrVU1W1WzMzMzPVUtSctTr6GQ5BgGgfCxqvo0QFU9UFX7q+qfgMv58RTRbmDd0OFrgT191idJerI+7z4KcAVwT1X98VD76qFubwR2duvbgE1JnpfkJcAG4La+6pMkHajPu49eC/wGcGeS7V3b7wPnJdnIYGroPuDtAFV1V5LrgLsZ3Ll0gXceSdJk9RYKVfUlRl8nuPEQx1wMXNxXTZKkQ/MTzZKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLU9PnNa88Kr/7da6ddgpag2//ordMuQZoKRwqSpMZQkCQ1Y4VCkpvHaZMkPbsdMhSSHJvkJGBVkhOTnNQt64EXHebYdUm+kOSeJHcleWfXflKSm5J8o3s9sWtPkg8n2ZVkR5JXHZkfUZI0rsONFN4O3A68rHtdWG4A/vQwx+4DfqeqfhY4DbggycnAhcDNVbUBuLnbBngDsKFbtgCXPeWfRpL0jBzy7qOq+hDwoST/uao+8lROXFV7gb3d+qNJ7gHWAGcDp3fdrgFuAX6va7+2qgr4SpITkqzuziNJmoCxbkmtqo8k+UVg/fAxVTXW/ZzddNMpwK3ACxd+0VfV3iQv6LqtAe4fOmx31/akUEiyhcFIghe/+MXjvL0kaUxjhUKSjwIvBbYD+7vmAg4bCkmeD1wPvKuqHkly0K4j2uqAhqqtwFaA2dnZA/ZLkp6+cT+8Nguc3E3tjC3JMQwC4WNV9emu+YGFaaEkq4EHu/bdwLqhw9cCe57K+0mSnplxP6ewE/jnT+XEGQwJrgDuqao/Htq1DdjcrW9mcNF6of2t3V1IpwEPez1BkiZr3JHCKuDuJLcBTyw0VtV/PMQxrwV+A7gzyfau7feB9wPXJTkf+Dbw5m7fjcBZwC7gceBt4/4QkqQjY9xQeO9TPXFVfYnR1wkAXjeifwEXPNX3kSQdOePeffR3fRciSZq+ce8+epQf3wn0XOAY4IdV9ZN9FSZJmrxxRwrHD28nOQc4tZeKJElT87SeklpVfwmccYRrkSRN2bjTR28a2nwOg88t+MExSTrKjHv30a8Nre8D7mPwrCJJ0lFk3GsKfmZAkpaBcb9kZ22SzyR5MMkDSa5Psrbv4iRJkzXuhearGDyG4kUMnlz6V12bJOkoMm4ozFTVVVW1r1uuBmZ6rEuSNAXjhsJ3k/x6khXd8uvA9/osTJI0eeOGwn8C3gL8XwZfenMuPrBOko46496S+ofA5qp6CCDJScAlDMJCknSUGHek8PMLgQBQVd9n8PWakqSjyLih8JwkJy5sdCOFcUcZkqRniXF/sX8Q+HKSTzF4vMVbgIt7q0qSNBXjfqL52iRzDB6CF+BNVXV3r5VJkiZu7CmgLgQMAkk6ij2tR2dLko5OhoIkqektFJJc2T1Ab+dQ23uTfCfJ9m45a2jfRUl2Jbk3yev7qkuSdHB9jhSuBs4c0X5pVW3slhsBkpwMbAJe3h3zZ0lW9FibJGmE3kKhqr4IfH/M7mcDn6iqJ6rqW8Au/A5oSZq4aVxTeEeSHd300sIH4tYA9w/12d21HSDJliRzSebm5+f7rlWSlpVJh8JlwEuBjQwerPfBrj0j+o78Duiq2lpVs1U1OzPj07sl6UiaaChU1QNVtb+q/gm4nB9PEe0G1g11XQvsmWRtkqQJh0KS1UObbwQW7kzaBmxK8rwkLwE2ALdNsjZJUo8PtUvyceB0YFWS3cB7gNOTbGQwNXQf8HaAqroryXUMPjG9D7igqvb3VZskabTeQqGqzhvRfMUh+l+MD9mTpKnyE82SpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJTW+hkOTKJA8m2TnUdlKSm5J8o3s9sWtPkg8n2ZVkR5JX9VWXJOng+hwpXA2cuajtQuDmqtoA3NxtA7wB2NAtW4DLeqxLknQQvYVCVX0R+P6i5rOBa7r1a4BzhtqvrYGvACckWd1XbZKk0SZ9TeGFVbUXoHt9Qde+Brh/qN/uru0ASbYkmUsyNz8/32uxkrTcLJULzRnRVqM6VtXWqpqtqtmZmZmey5Kk5WXSofDAwrRQ9/pg174bWDfUby2wZ8K1SdKyN+lQ2AZs7tY3AzcMtb+1uwvpNODhhWkmSdLkrOzrxEk+DpwOrEqyG3gP8H7guiTnA98G3tx1vxE4C9gFPA68ra+6JEkH11soVNV5B9n1uhF9C7igr1okSeNZKheaJUlLgKEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJKaldN40yT3AY8C+4F9VTWb5CTgk8B64D7gLVX10DTqk6TlapojhV+uqo1VNdttXwjcXFUbgJu7bUnSBC2l6aOzgWu69WuAc6ZYiyQtS9MKhQL+JsntSbZ0bS+sqr0A3esLRh2YZEuSuSRz8/PzEypXkpaHqVxTAF5bVXuSvAC4KcnXxz2wqrYCWwFmZ2errwIlaTmaykihqvZ0rw8CnwFOBR5Ishqge31wGrVJ0nI28VBI8hNJjl9YB/49sBPYBmzuum0Gbph0bZK03E1j+uiFwGeSLLz/X1TV55J8FbguyfnAt4E3T6E2SVrWJh4KVfVN4JUj2r8HvG7S9UiSfmwp3ZIqSZoyQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDVLLhSSnJnk3iS7klw47XokaTlZUqGQZAXwp8AbgJOB85KcPN2qJGn5WFKhAJwK7Kqqb1bV/wM+AZw95ZokadlYOe0CFlkD3D+0vRv4heEOSbYAW7rNx5LcO6HaloNVwHenXcRSkEs2T7sEPZn/Nhe8J0fiLD99sB1LLRRG/bT1pI2qrcDWyZSzvCSZq6rZadchLea/zclZatNHu4F1Q9trgT1TqkWSlp2lFgpfBTYkeUmS5wKbgG1TrkmSlo0lNX1UVfuSvAP4n8AK4MqqumvKZS0nTstpqfLf5oSkqg7fS5K0LCy16SNJ0hQZCpKkxlBY5pJUko8Oba9MMp/ks9OsSwJIsj/J9iRfS3JHkl+cdk1HuyV1oVlT8UPgFUmOq6p/BP4d8J0p1yQt+Meq2giQ5PXAfwd+abolHd0cKQjgr4Ff7dbPAz4+xVqkg/lJ4KFpF3G0MxQEg2dMbUpyLPDzwK1TrkdacFw3ffR14M+BP5x2QUc7p49EVe1Isp7BKOHG6VYjPcnw9NG/Bq5N8oryXvreOFLQgm3AJTh1pCWqqv6ewYPxZqZdy9HMkYIWXAk8XFV3Jjl92sVIiyV5GYMnHXxv2rUczQwFAVBVu4EPTbsOaZHjkmzv1gNsrqr90yzoaOdjLiRJjdcUJEmNoSBJagwFSVJjKEiSGkNBktR4S6rUSfJe4DEGz9j5YlV9foq1vG/aNWh5MhSkRarq3dag5crpIy1rSf5LknuTfB74V13b1UnO7dbfneSrSXYm2ZokXftrkuxI8vdJ/ijJzq79N5N8Osnnknwjyf8Yeq/zktzZnesDXduK7v12dvt+e0QN709yd/d+l0z0P5CWHUcKWraSvBrYBJzC4P+FO4DbF3X7k6p6X9f/o8B/AP4KuArYUlVfTvL+Rcds7M75BHBvko8A+4EPAK9m8Pjnv0lyDnA/sKaqXtG9xwmLajwJeCPwsqqqxfulI82RgpazfwN8pqoer6pHGDwUcLFfTnJrkjuBM4CXd7+Yj6+qL3d9/mLRMTdX1cNV9SPgbuCngdcAt1TVfFXtAz4G/Fvgm8C/SPKRJGcCjyw61yPAj4A/T/Im4PFn/FNLh2AoaLk76HNeuu+X+DPg3Kr6OeBy4FgGz+A5lCeG1vczGIWMPKaqHgJeCdwCXMDgOwOG9+8DTgWuB84BPneY95aeEUNBy9kXgTcmOS7J8cCvLdp/bPf63STPB86F9ov80SSndfs3jfFetwK/lGRVkhUMvrvi75KsAp5TVdcD/xV41fBB3fv+VFXdCLyLwdSU1BuvKWjZqqo7knwS2A78A/C/Fu3/QZLLgTuB+4CvDu0+H7g8yQ8Z/JX/8GHea2+Si4AvMBg13FhVNyR5JXBVkoU/0C5adOjxwA3dqCXAbz/lH1R6CnxKqvQ0JHl+VT3WrV8IrK6qd065LOkZc6QgPT2/2v3lv5LBKOM3p1uOdGQ4UpAkNV5oliQ1hoIkqTEUJEmNoSBJagwFSVLz/wFBxb5aGDaWTgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(dataset['diagnosis'],label= 'Count')\n",
    "B,M = dataset['diagnosis'].value_counts() \n",
    "print(\"Benign = {}\".format(B)) \n",
    "print(\"Malignanat = {}\".format(M)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete last Column because it contain all NAN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset['Unnamed: 32']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work on Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Devide Data into dependent and independent Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.iloc[:,2:].values\n",
    "\n",
    "y = dataset.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Encoding Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder_x_1 = LabelEncoder()\n",
    "\n",
    "y = labelencoder_x_1.fit_transform(y)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Spliting the dataset into Traning set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.1,random_state=0) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Feauture Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.52787029,  2.49821982, -0.59939466, ..., -1.74713139,\n",
       "        -0.79044533, -0.91054389],\n",
       "       [-0.55333608,  0.29431013, -0.60759343, ..., -0.62275667,\n",
       "        -0.33646358, -0.83551633],\n",
       "       [ 2.15452653,  0.40392257,  2.26525805, ...,  1.03846122,\n",
       "        -0.11504791,  0.26488788],\n",
       "       ...,\n",
       "       [-1.3297598 , -0.21876938, -1.32088704, ..., -0.98271999,\n",
       "        -0.718764  , -0.13637062],\n",
       "       [-1.24940108, -0.24209117, -1.2835826 , ..., -1.74713139,\n",
       "        -1.58690456, -1.01280367],\n",
       "       [-0.74291476,  1.08958336, -0.71827692, ..., -0.2865488 ,\n",
       "        -1.26354211,  0.19486216]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19207516,  0.21655218, -0.11542614, ...,  1.60315155,\n",
       "         1.38812047,  1.39561107],\n",
       "       [-0.24797128,  1.29875111, -0.31114085, ..., -0.76400211,\n",
       "        -0.79834577, -0.93437281],\n",
       "       [-0.00967731, -0.89539007, -0.07447263, ..., -0.40317641,\n",
       "        -1.38855752, -0.97661799],\n",
       "       ...,\n",
       "       [-0.46861385,  0.07241152, -0.46935075, ..., -0.49962508,\n",
       "        -0.33652424, -0.40576652],\n",
       "       [-0.75986205, -0.81073603, -0.78878818, ..., -0.67538685,\n",
       "        -0.01842311,  0.13529668],\n",
       "       [-0.80399056,  1.71744542, -0.84310547, ..., -1.40405566,\n",
       "        -0.02225565, -0.77243298]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras Libeary Work Start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the First Input and first hidden layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=30, units=16, kernel_initializer=\"uniform\")`\n",
      "  \"\"\"\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=16, kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# Adding the first input and first hidden layear\n",
    "\n",
    "clssifier = Sequential()\n",
    "\n",
    "clssifier.add(Dense(output_dim=16,init='uniform',activation='relu',input_dim=30))\n",
    "\n",
    "\n",
    "#Adding Second hidden layer\n",
    "clssifier.add(Dense(output_dim=16,init='uniform',activation='relu'))\n",
    "\n",
    "#Adding the Output layer\n",
    "clssifier.add(Dense(output_dim=1,init='uniform',activation='sigmoid')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Compile classifier for how to use optimizer weight, Which loss function it contain and how to show the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clssifier.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Our ANN with traning and testing with diffrent ephoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.6927 - accuracy: 0.6719\n",
      "Epoch 2/150\n",
      "512/512 [==============================] - 0s 72us/step - loss: 0.6908 - accuracy: 0.7617\n",
      "Epoch 3/150\n",
      "512/512 [==============================] - 0s 32us/step - loss: 0.6875 - accuracy: 0.8691\n",
      "Epoch 4/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.6818 - accuracy: 0.9160\n",
      "Epoch 5/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.6725 - accuracy: 0.9238\n",
      "Epoch 6/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.6585 - accuracy: 0.9336\n",
      "Epoch 7/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.6378 - accuracy: 0.9336\n",
      "Epoch 8/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.6104 - accuracy: 0.9336\n",
      "Epoch 9/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.5779 - accuracy: 0.9355\n",
      "Epoch 10/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.5412 - accuracy: 0.9355\n",
      "Epoch 11/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.5038 - accuracy: 0.9395\n",
      "Epoch 12/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.4659 - accuracy: 0.9434\n",
      "Epoch 13/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.4292 - accuracy: 0.9531\n",
      "Epoch 14/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.3935 - accuracy: 0.9531\n",
      "Epoch 15/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.3592 - accuracy: 0.9590\n",
      "Epoch 16/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.3258 - accuracy: 0.9590\n",
      "Epoch 17/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.2944 - accuracy: 0.9609\n",
      "Epoch 18/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.2649 - accuracy: 0.9648\n",
      "Epoch 19/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.2377 - accuracy: 0.9668\n",
      "Epoch 20/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.2146 - accuracy: 0.9668\n",
      "Epoch 21/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.1944 - accuracy: 0.9707\n",
      "Epoch 22/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.1771 - accuracy: 0.9707\n",
      "Epoch 23/150\n",
      "512/512 [==============================] - 0s 56us/step - loss: 0.1625 - accuracy: 0.9727\n",
      "Epoch 24/150\n",
      "512/512 [==============================] - 0s 39us/step - loss: 0.1502 - accuracy: 0.9727\n",
      "Epoch 25/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.1396 - accuracy: 0.9746\n",
      "Epoch 26/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.1312 - accuracy: 0.9766\n",
      "Epoch 27/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.1236 - accuracy: 0.9785\n",
      "Epoch 28/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.1177 - accuracy: 0.9785\n",
      "Epoch 29/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.1120 - accuracy: 0.9785\n",
      "Epoch 30/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.1070 - accuracy: 0.9785\n",
      "Epoch 31/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.1024 - accuracy: 0.9766\n",
      "Epoch 32/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0983 - accuracy: 0.9805\n",
      "Epoch 33/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0947 - accuracy: 0.9805\n",
      "Epoch 34/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0918 - accuracy: 0.9805\n",
      "Epoch 35/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0890 - accuracy: 0.9805\n",
      "Epoch 36/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0866 - accuracy: 0.9805\n",
      "Epoch 37/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0845 - accuracy: 0.9805\n",
      "Epoch 38/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0826 - accuracy: 0.9824\n",
      "Epoch 39/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0807 - accuracy: 0.9824\n",
      "Epoch 40/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0790 - accuracy: 0.9824\n",
      "Epoch 41/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0774 - accuracy: 0.9824\n",
      "Epoch 42/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0759 - accuracy: 0.9824\n",
      "Epoch 43/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0745 - accuracy: 0.9824\n",
      "Epoch 44/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0733 - accuracy: 0.9824\n",
      "Epoch 45/150\n",
      "512/512 [==============================] - 0s 45us/step - loss: 0.0722 - accuracy: 0.9844\n",
      "Epoch 46/150\n",
      "512/512 [==============================] - 0s 32us/step - loss: 0.0709 - accuracy: 0.9844\n",
      "Epoch 47/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0698 - accuracy: 0.9844\n",
      "Epoch 48/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0688 - accuracy: 0.9824\n",
      "Epoch 49/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0680 - accuracy: 0.9824\n",
      "Epoch 50/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0670 - accuracy: 0.9824\n",
      "Epoch 51/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0661 - accuracy: 0.9824\n",
      "Epoch 52/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0651 - accuracy: 0.9844\n",
      "Epoch 53/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0641 - accuracy: 0.9844\n",
      "Epoch 54/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0633 - accuracy: 0.9844\n",
      "Epoch 55/150\n",
      "512/512 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.99 - 0s 31us/step - loss: 0.0627 - accuracy: 0.9844\n",
      "Epoch 56/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0621 - accuracy: 0.9844\n",
      "Epoch 57/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0614 - accuracy: 0.9844\n",
      "Epoch 58/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0608 - accuracy: 0.9844\n",
      "Epoch 59/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0602 - accuracy: 0.9844\n",
      "Epoch 60/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0595 - accuracy: 0.9844\n",
      "Epoch 61/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0590 - accuracy: 0.9844\n",
      "Epoch 62/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0584 - accuracy: 0.9844\n",
      "Epoch 63/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0580 - accuracy: 0.9844\n",
      "Epoch 64/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0575 - accuracy: 0.9844\n",
      "Epoch 65/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0571 - accuracy: 0.9844\n",
      "Epoch 66/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0566 - accuracy: 0.9844\n",
      "Epoch 67/150\n",
      "512/512 [==============================] - 0s 39us/step - loss: 0.0562 - accuracy: 0.9844\n",
      "Epoch 68/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0557 - accuracy: 0.9844\n",
      "Epoch 69/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0553 - accuracy: 0.9844\n",
      "Epoch 70/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0549 - accuracy: 0.9844\n",
      "Epoch 71/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0545 - accuracy: 0.9844\n",
      "Epoch 72/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0541 - accuracy: 0.9844\n",
      "Epoch 73/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0538 - accuracy: 0.9844\n",
      "Epoch 74/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0535 - accuracy: 0.9844\n",
      "Epoch 75/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0532 - accuracy: 0.9844\n",
      "Epoch 76/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0529 - accuracy: 0.9844\n",
      "Epoch 77/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0526 - accuracy: 0.9844\n",
      "Epoch 78/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0523 - accuracy: 0.9844\n",
      "Epoch 79/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0520 - accuracy: 0.9844\n",
      "Epoch 80/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0517 - accuracy: 0.9844\n",
      "Epoch 81/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0514 - accuracy: 0.9844\n",
      "Epoch 82/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0511 - accuracy: 0.9844\n",
      "Epoch 83/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0508 - accuracy: 0.9844\n",
      "Epoch 84/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0506 - accuracy: 0.9844\n",
      "Epoch 85/150\n",
      "512/512 [==============================] - 0s 16us/step - loss: 0.0503 - accuracy: 0.9844\n",
      "Epoch 86/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0500 - accuracy: 0.9863\n",
      "Epoch 87/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0498 - accuracy: 0.9863\n",
      "Epoch 88/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0495 - accuracy: 0.9883\n",
      "Epoch 89/150\n",
      "512/512 [==============================] - 0s 41us/step - loss: 0.0493 - accuracy: 0.9883\n",
      "Epoch 90/150\n",
      "512/512 [==============================] - 0s 32us/step - loss: 0.0490 - accuracy: 0.9883\n",
      "Epoch 91/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0487 - accuracy: 0.9883\n",
      "Epoch 92/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0486 - accuracy: 0.9883\n",
      "Epoch 93/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0482 - accuracy: 0.9883\n",
      "Epoch 94/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0479 - accuracy: 0.9883\n",
      "Epoch 95/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0477 - accuracy: 0.9902\n",
      "Epoch 96/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0474 - accuracy: 0.9902\n",
      "Epoch 97/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0472 - accuracy: 0.9902\n",
      "Epoch 98/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0469 - accuracy: 0.9883\n",
      "Epoch 99/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0467 - accuracy: 0.9883\n",
      "Epoch 100/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0464 - accuracy: 0.9883\n",
      "Epoch 101/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0462 - accuracy: 0.9883\n",
      "Epoch 102/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0460 - accuracy: 0.9883\n",
      "Epoch 103/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0458 - accuracy: 0.9863\n",
      "Epoch 104/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0456 - accuracy: 0.9863\n",
      "Epoch 105/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0454 - accuracy: 0.9883\n",
      "Epoch 106/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0451 - accuracy: 0.9883\n",
      "Epoch 107/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0449 - accuracy: 0.9883\n",
      "Epoch 108/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0447 - accuracy: 0.9902\n",
      "Epoch 109/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0445 - accuracy: 0.9902\n",
      "Epoch 110/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0444 - accuracy: 0.9902\n",
      "Epoch 111/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0441 - accuracy: 0.9922\n",
      "Epoch 112/150\n",
      "512/512 [==============================] - 0s 40us/step - loss: 0.0440 - accuracy: 0.9922\n",
      "Epoch 113/150\n",
      "512/512 [==============================] - 0s 48us/step - loss: 0.0440 - accuracy: 0.9922\n",
      "Epoch 114/150\n",
      "512/512 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 1.00 - 0s 31us/step - loss: 0.0438 - accuracy: 0.9922\n",
      "Epoch 115/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0435 - accuracy: 0.9922\n",
      "Epoch 116/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0432 - accuracy: 0.9922\n",
      "Epoch 117/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0430 - accuracy: 0.9922\n",
      "Epoch 118/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0427 - accuracy: 0.9922\n",
      "Epoch 119/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0425 - accuracy: 0.9922\n",
      "Epoch 120/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0423 - accuracy: 0.9922\n",
      "Epoch 121/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0421 - accuracy: 0.9922\n",
      "Epoch 122/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0419 - accuracy: 0.9922\n",
      "Epoch 123/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0418 - accuracy: 0.9922\n",
      "Epoch 124/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0417 - accuracy: 0.9922\n",
      "Epoch 125/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0415 - accuracy: 0.9922\n",
      "Epoch 126/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0413 - accuracy: 0.9922\n",
      "Epoch 127/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0413 - accuracy: 0.9922\n",
      "Epoch 128/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0411 - accuracy: 0.9922\n",
      "Epoch 129/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0409 - accuracy: 0.9922\n",
      "Epoch 130/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0407 - accuracy: 0.9922\n",
      "Epoch 131/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0405 - accuracy: 0.9922\n",
      "Epoch 132/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0403 - accuracy: 0.9922\n",
      "Epoch 133/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0400 - accuracy: 0.9922\n",
      "Epoch 134/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0399 - accuracy: 0.9922\n",
      "Epoch 135/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0397 - accuracy: 0.9922\n",
      "Epoch 136/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0395 - accuracy: 0.9922\n",
      "Epoch 137/150\n",
      "512/512 [==============================] - 0s 35us/step - loss: 0.0393 - accuracy: 0.9922\n",
      "Epoch 138/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0392 - accuracy: 0.9922\n",
      "Epoch 139/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0391 - accuracy: 0.9922\n",
      "Epoch 140/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0389 - accuracy: 0.9922\n",
      "Epoch 141/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0388 - accuracy: 0.9922\n",
      "Epoch 142/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0387 - accuracy: 0.9922\n",
      "Epoch 143/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0386 - accuracy: 0.9922\n",
      "Epoch 144/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0385 - accuracy: 0.9922\n",
      "Epoch 145/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0384 - accuracy: 0.9922\n",
      "Epoch 146/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0383 - accuracy: 0.9922\n",
      "Epoch 147/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0380 - accuracy: 0.9922\n",
      "Epoch 148/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0379 - accuracy: 0.9922\n",
      "Epoch 149/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0378 - accuracy: 0.9922\n",
      "Epoch 150/150\n",
      "512/512 [==============================] - 0s 47us/step - loss: 0.0377 - accuracy: 0.9922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x24f132b85c0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clssifier.fit(x_train,y_train,batch_size=100,nb_epoch=150) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19207516,  0.21655218, -0.11542614, ...,  1.60315155,\n",
       "         1.38812047,  1.39561107],\n",
       "       [-0.24797128,  1.29875111, -0.31114085, ..., -0.76400211,\n",
       "        -0.79834577, -0.93437281],\n",
       "       [-0.00967731, -0.89539007, -0.07447263, ..., -0.40317641,\n",
       "        -1.38855752, -0.97661799],\n",
       "       ...,\n",
       "       [-0.46861385,  0.07241152, -0.46935075, ..., -0.49962508,\n",
       "        -0.33652424, -0.40576652],\n",
       "       [-0.75986205, -0.81073603, -0.78878818, ..., -0.67538685,\n",
       "        -0.01842311,  0.13529668],\n",
       "       [-0.80399056,  1.71744542, -0.84310547, ..., -1.40405566,\n",
       "        -0.02225565, -0.77243298]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the Test set reults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clssifier.predict(x_test)  \n",
    "y_pred = (y_pred > 0.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "cm = confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To See the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(cm,annot=True)\n",
    "plt.savefig('h.png')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
